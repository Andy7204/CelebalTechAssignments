{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWZrYRRt9CnHGktTRvxn03",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andy7204/CelebalTechAssignments/blob/main/House_Price_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtW-zWKyEoDg",
        "outputId": "b34c3119-c773-40e4-fac4-ad825f4fdc56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… submission.csv saved!\n",
            "ðŸ“Š XGBoost CV RMSE: 0.1179\n"
          ]
        }
      ],
      "source": [
        "# ======================================\n",
        "# 1. IMPORT LIBRARIES\n",
        "# ======================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# ======================================\n",
        "# 2. LOAD DATA\n",
        "# ======================================\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "test_ids = test_df['Id']\n",
        "\n",
        "# ======================================\n",
        "# 3. DROP HEAVILY MISSING FEATURES\n",
        "# ======================================\n",
        "threshold = 0.3\n",
        "missing_cols = train_df.columns[train_df.isnull().mean() > threshold]\n",
        "train_df.drop(columns=missing_cols, inplace=True)\n",
        "test_df.drop(columns=missing_cols, inplace=True)\n",
        "\n",
        "# ======================================\n",
        "# 4. TARGET + CONCATENATE FOR PREPROCESSING\n",
        "# ======================================\n",
        "y = np.log1p(train_df['SalePrice'])  # Log-transform target for regression\n",
        "train_df.drop(columns=['SalePrice'], inplace=True)\n",
        "\n",
        "train_df['source'] = 'train'\n",
        "test_df['source'] = 'test'\n",
        "combined = pd.concat([train_df, test_df], axis=0)\n",
        "combined.drop(columns=['Id'], inplace=True)\n",
        "\n",
        "# ======================================\n",
        "# 5. IMPUTE MISSING VALUES\n",
        "# ======================================\n",
        "numeric_cols = combined.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_cols = combined.select_dtypes(include='object').columns\n",
        "\n",
        "combined[numeric_cols] = SimpleImputer(strategy='median').fit_transform(combined[numeric_cols])\n",
        "combined[categorical_cols] = SimpleImputer(strategy='most_frequent').fit_transform(combined[categorical_cols])\n",
        "\n",
        "# ======================================\n",
        "# 6. ONE-HOT ENCODING\n",
        "# ======================================\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"cat\", OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)\n",
        "], remainder='passthrough')\n",
        "\n",
        "combined_encoded = preprocessor.fit_transform(combined)\n",
        "train_data = combined_encoded[combined['source'] == 'train']\n",
        "test_data = combined_encoded[combined['source'] == 'test']\n",
        "\n",
        "# ======================================\n",
        "# 7. OUTLIER REMOVAL (based on GrLivArea)\n",
        "# ======================================\n",
        "# Optional: remove extreme outliers in training data\n",
        "train_data_df = pd.DataFrame(train_data)\n",
        "outliers = train_data_df.iloc[:, list(combined.columns).index('GrLivArea')] > 4000\n",
        "train_data = train_data[~outliers]\n",
        "y = y[~outliers]\n",
        "\n",
        "# ======================================\n",
        "# 8. SCALING AND PCA\n",
        "# ======================================\n",
        "scaler = StandardScaler()\n",
        "train_scaled = scaler.fit_transform(train_data)\n",
        "test_scaled = scaler.transform(test_data)\n",
        "\n",
        "pca = PCA(n_components=0.95, random_state=42)\n",
        "X_train_pca = pca.fit_transform(train_scaled)\n",
        "X_test_pca = pca.transform(test_scaled)\n",
        "\n",
        "# ======================================\n",
        "# 9. TRAIN MODELS\n",
        "# ======================================\n",
        "\n",
        "# Lasso\n",
        "lasso = make_pipeline(RobustScaler(), LassoCV(cv=5, random_state=42))\n",
        "lasso.fit(train_scaled, y)\n",
        "lasso_preds = lasso.predict(test_scaled)\n",
        "\n",
        "# XGBoost\n",
        "xgb = XGBRegressor(n_estimators=1000, learning_rate=0.05, max_depth=3,\n",
        "                   subsample=0.7, colsample_bytree=0.7, random_state=42)\n",
        "xgb.fit(train_scaled, y)\n",
        "xgb_preds = xgb.predict(test_scaled)\n",
        "\n",
        "# ======================================\n",
        "# 10. ENSEMBLE + EXPORT SUBMISSION\n",
        "# ======================================\n",
        "final_preds = 0.6 * np.expm1(xgb_preds) + 0.4 * np.expm1(lasso_preds)\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'Id': test_ids,\n",
        "    'SalePrice': final_preds\n",
        "})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"âœ… submission.csv saved!\")\n",
        "\n",
        "# ======================================\n",
        "# 11. (OPTIONAL) EVALUATE RMSE VIA CV\n",
        "# ======================================\n",
        "scores = cross_val_score(xgb, train_scaled, y, scoring='neg_root_mean_squared_error', cv=5)\n",
        "print(f\"ðŸ“Š XGBoost CV RMSE: {-np.mean(scores):.4f}\")\n"
      ]
    }
  ]
}